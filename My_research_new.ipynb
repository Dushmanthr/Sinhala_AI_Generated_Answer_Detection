{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxkd2fzw/cchhqlWGK73rz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dushmanthr/Sinhala_AI_Generated_Answer_Detection/blob/main/My_research_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frCKoc3kayfX"
      },
      "outputs": [],
      "source": [
        "!pip install sinling\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install advertools"
      ],
      "metadata": {
        "id": "aMtk_IQpgAbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install translate"
      ],
      "metadata": {
        "id": "IDwb-Gqughph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyenchant"
      ],
      "metadata": {
        "id": "kK5nQX7jhC0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sinling import SinhalaTokenizer as tokenizer,SinhalaStemmer as stemmer, POSTagger,preprocess, word_joiner,word_splitter\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize,TweetTokenizer\n",
        "from nltk.probability import FreqDist\n",
        "import advertools as adv\n",
        "from pathlib import Path\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
        "from sklearn import linear_model\n",
        "\n",
        "import codecs\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "metadata": {
        "id": "kaMhHQ3Eh6E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "34TLfAiqiO2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% read csv file data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "file = pd.read_excel('Processed.xlsx')\n",
        "#pd.read_csv(Path().joinpath('data','traindata2.csv'))\n",
        "print(file['Answers'].head(11))\n",
        "plt.rcParams['font.sans-serif']= \"cmr10\"\n",
        "sns.countplot(x='Label',hue='Label',data=file)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MQLT9t3-je97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('Processed.xlsx')\n",
        "\n",
        "print(df['Answers'])"
      ],
      "metadata": {
        "id": "ZFH5BstQpjOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = [\"සහ\",\"සමග\",\"සමඟ\",\"අහා\",\"ආහ්\",\"ආ\",\"ඕහෝ\",\"අනේ\",\"අඳෝ\",\"අපොයි\",\"පෝ\",\"අයියෝ\",\"ආයි\",\"ඌයි\",\"චී\",\"චිහ්\",\"චික්\",\"හෝ‍\",\"දෝ\",\n",
        "                 \"දෝහෝ\",\"මෙන්\",\"සේ\",\"වැනි\",\"බඳු\",\"වන්\",\"අයුරු\",\"අයුරින්\",\"ලෙස\",\"වැඩි\",\"ශ්‍රී\",\"හා\",\"ය\",\"නිසා\",\"නිසාවෙන්\",\"බවට\",\"බව\",\"බවෙන්\",\"නම්\",\"වැඩි\",\"සිට\",\n",
        "                 \"දී\",\"මහා\",\"මහ\",\"පමණ\",\"පමණින්\",\"පමන\",\"වන\",\"විට\",\"විටින්\",\"මේ\",\"මෙලෙස\",\"මෙයින්\",\"ඇති\",\"ලෙස\",\"සිදු\",\"වශයෙන්\",\"යන\",\"සඳහා\",\"මගින්\",\"හෝ‍\",\n",
        "                 \"ඉතා\",\"ඒ\",\"එම\",\"ද\",\"අතර\",\"විසින්\",\"සමග\",\"පිළිබඳව\",\"පිළිබඳ\",\"තුළ\",\"බව\",\"වැනි\",\"මහ\",\"මෙම\",\"මෙහි\",\"මේ\",\"වෙත\",\"වෙතින්\",\"වෙතට\",\"වෙනුවෙන්\",\n",
        "                 \"වෙනුවට\",\"වෙන\",\"ගැන\",\"නෑ\",\"අනුව\",\"නව\",\"පිළිබඳ\",\"විශේෂ\",\"දැනට\",\"එහෙන්\",\"මෙහෙන්\",\"එහේ\",\"මෙහේ\",\"ම\",\"තවත්\",\"තව\",\"සහ\",\"දක්වා\",\"ට\",\"ගේ\",\n",
        "                 \"එ\",\"ක\",\"ක්\",\"බවත්\",\"බවද\",\"මත\",\"ඇතුලු\",\"ඇතුළු\",\"මෙසේ\",\"වඩා\",\"වඩාත්ම\",\"නිති\",\"නිතිත්\",\"නිතොර\",\"නිතර\",\"ඉක්බිති\",\"දැන්\",\"යලි\",\"පුන\",\"ඉතින්\",\n",
        "                 \"සිට\",\"සිටන්\",\"පටන්\",\"තෙක්\",\"දක්වා\",\"සා\",\"තාක්\",\"තුවක්\",\"පවා\",\"ද\",\"හෝ‍\",\"වත්\",\"විනා\",\"හැර\",\"මිස\",\"මුත්\",\"කිම\",\"කිම්\",\"ඇයි\",\"මන්ද\",\"හෙවත්\",\n",
        "                 \"නොහොත්\",\"පතා\",\"පාසා\",\"ගානෙ\",\"තව\",\"ඉතා\",\"බොහෝ\",\"වහා\",\"සෙද\",\"සැනින්\",\"හනික\",\"එම්බා\",\"එම්බල\",\"බොල\",\"නම්\",\"වනාහි\",\"කලී\",\"ඉඳුරා\",\n",
        "                 \"අන්න\",\"ඔන්න\",\"මෙන්න\",\"උදෙසා\",\"පිණිස\",\"සඳහා\",\"රබයා\",\"නිසා\",\"එනිසා\",\"එබැවින්\",\"බැවින්\",\"හෙයින්\",\"සේක්\",\"සේක\",\"ගැන\",\"අනුව\",\"පරිදි\",\"විට\",\n",
        "                 \"තෙක්\",\"මෙතෙක්\",\"මේතාක්\",\"තුරු\",\"තුරා\",\"තුරාවට\",\"තුලින්\",\"නමුත්\",\"එනමුත්\",\"වස්\",'මෙන්',\"ලෙස\",\"පරිදි\",\"එහෙත්\"]\n"
      ],
      "metadata": {
        "id": "p5exibbc7aRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords_set)"
      ],
      "metadata": {
        "id": "S2JZq56Q7l2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_words_dict = {\n",
        "    \"unp\": \"එක්සත් ජාතික පක්ෂය\",\n",
        "    \"muslim\": \"මුස්ලිම්\",\n",
        "    \"srilankanpolitics\": \"ශ්‍රී ලංකන් දේශපාලනය\",\n",
        "    \"council\": \"සභාව\",\n",
        "    \"sinhala\": \"සිංහල\",\n",
        "    \"buddhist\": \"බෞද්ධ\",\n",
        "    \"buddhism\": \"බුද්ධාගම\",\n",
        "    \"srilanka\": \"ශ්‍රී ලංකාව\",\n",
        "    \"racist\": \"ජාතිවාදී\",\n",
        "    \"presidentialfirst\": \"පළමු ජනාධිපති\",\n",
        "    \"feeling\": \"හැඟීම\",\n",
        "    \"feminist\": \"ස්ත්‍රීවාදී\",\n",
        "    \"loved\": \"ආදරය කළා\",\n",
        "    \"team\": \"කණ්ඩායම\",\n",
        "    \"tclsl\":\"ට්විටර් ක්‍රිකට් ලීගය ශ්‍රී ලංකාව\",\n",
        "    \"pongal\": \"පොංගල්\",\n",
        "    \"pongalfestival\": \"පොංගල් උත්සවය\",\n",
        "    \"women\": \"කාන්තා\",\n",
        "    \"nextpresidentinsl\": \"ශ්‍රී ලංකාවේ මීළඟ ජනාධිපති \",\n",
        "    \"seventhexecutivepresident\": \"හත්වන විධායක සභාපති\",\n",
        "    \"hate\": \"වෛරය\",\n",
        "    \"love\": \"ආදරය\",\n",
        "    \"angry\": \"තරහයි\",\n",
        "    \"doctor\": \"ඩොක්ටර්\",\n",
        "    \"ltte\": \"එල්ටීටීඊය\",\n",
        "    \"lka\": \"‍ශ්‍රී ලංකාව\",\n",
        "    \"hurt\": \"රිදෙනවා\",\n",
        "    \"typo\": \"යතුරු ලියනය\",\n",
        "    \"racial\": \"වාර්ගික\",\n",
        "    \"hatred\": \"වෛරය\",\n",
        "    \"halal\": \"හලාල්\",\n",
        "    \"wicket\": \"කඩුල්ල\",\n",
        "    \"taker\": \"ටේකර්\",\n",
        "    \"indoor\": \"ගෘහස්ථ\",\n",
        "    \"attacker\": \"ප්‍රහාරකයා\",\n",
        "    \"attack\": \"ප්රහාරය\",\n",
        "    \"spikers\": \"ස්පිකර්ස්\",\n",
        "    \"training\": \"පුහුණුව\",\n",
        "    \"final\": \"අවසාන\",\n",
        "    \"match\": \"තරගය\",\n",
        "    \"tournament\": \"තරඟාවලිය\",\n",
        "    \"youth\": \"තරුණ\",\n",
        "    \"amen\": \"ආමෙන්\",\n",
        "    \"enough\": \"ඇති\",\n",
        "    \"standagainstracism\": \"ජාතිවාදයට එරෙහිව නැගී සිටින්න\"\n",
        "}\n",
        "\n",
        "def translate_to_sinhala(word: str) -> str:\n",
        "  word = word.lower()\n",
        "  if word in translate_words_dict:\n",
        "        return translate_words_dict[word]\n",
        "  return word"
      ],
      "metadata": {
        "id": "Yq4hqLTk_LGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sinling import SinhalaTokenizer as tokenizer,SinhalaStemmer as stemmer, POSTagger,preprocess, word_joiner,word_splitter\n",
        "stemmer = stemmer()\n",
        "\n",
        "def stem_word(word: str) -> str:\n",
        "    word= translate_to_sinhala(word)\n",
        "    \"\"\"\n",
        "    Stemming words\n",
        "    :param word: word\n",
        "    :return: stemmed word\n",
        "    \"\"\"\n",
        "    if len(word) < 4:\n",
        "        return word\n",
        "\n",
        "    # remove 'ට'\n",
        "    if word[-1] == 'ට':\n",
        "        return word[:-1]\n",
        "\n",
        "    # remove 'ම'\n",
        "    if word[-1] == 'ම':\n",
        "        return word[:-1]\n",
        "\n",
        "    # remove 'ද'\n",
        "    if word[-1] == 'ද':\n",
        "        return word[:-1]\n",
        "\n",
        "    # remove 'ටත්'\n",
        "    if word[-3:] == 'ටත්':\n",
        "        return word[:-3]\n",
        "\n",
        "    # remove 'එක්'\n",
        "    if word[-3:] == 'ෙක්':\n",
        "        return word[:-3]\n",
        "\n",
        "    # remove 'යේ'\n",
        "    if word[-2:] == 'යේ':\n",
        "        return word[:-2]\n",
        "\n",
        "    # remove 'ගෙ' (instead of ගේ because this step comes after simplifying text)\n",
        "    if word[-2:] == 'ගෙ':\n",
        "        return word[:-2]\n",
        "\n",
        "    # remove 'එ'\n",
        "    if word[-1:] == 'ෙ':\n",
        "        return word[:-1]\n",
        "\n",
        "    # remove 'ක්'\n",
        "    if word[-2:] == 'ක්':\n",
        "        return word[:-2]\n",
        "\n",
        "    # remove 'වත්'\n",
        "    if word[-3:] == 'වත්':\n",
        "        return word[:-3]\n",
        "\n",
        "    word=stemmer.stem(word)\n",
        "    word=word[0]\n",
        "\n",
        "\n",
        "    # else\n",
        "    return word"
      ],
      "metadata": {
        "id": "BhLs1C9Y_A3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def filter_stop_words(sentences):\n",
        "    filtered_sentences = []\n",
        "    detokenizer = Detok()\n",
        "    for sentence in sentences:\n",
        "        tokenized_sentence = word_tokenize(sentence)\n",
        "        filtered_sentence = [word for word in tokenized_sentence if word not in stopwords_set]\n",
        "        filtered_sentence = []\n",
        "        for w in tokenized_sentence:\n",
        "            if w not in stopwords_set:\n",
        "                filtered_sentence.append(stem_word(w))\n",
        "        filtered_sentences.append(filtered_sentence)\n",
        "    return filtered_sentences\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GNj26MNQ7992"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenize(filtered_sentences):\n",
        "    detokenized_sentences = []\n",
        "    for sentence in filtered_sentences:\n",
        "        detokenized_sentences.append(TreebankWordDetokenizer().detokenize(sentence))\n",
        "    return detokenized_sentences\n",
        "\n",
        "    print(detokenized_sentences)\n"
      ],
      "metadata": {
        "id": "zpUYnUoC-Oqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "epEbye9yDr6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Answers'] = df['Answers'].astype(str)"
      ],
      "metadata": {
        "id": "4Cr5kgwpEKg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "filtered_sentences = filter_stop_words(df['Answers'])\n",
        "detokenized_sentences = detokenize(filtered_sentences)"
      ],
      "metadata": {
        "id": "XxajLOkuDEDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detokenized_sentences"
      ],
      "metadata": {
        "id": "JUhV37hFERW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplify_characters_dict = {\n",
        "    # Consonant\n",
        "    \"ඛ\": \"ක\",\n",
        "    \"ඝ\": \"ග\",\n",
        "    \"ඟ\": \"ග\",\n",
        "    \"ඡ\": \"ච\",\n",
        "    \"ඣ\": \"ජ\",\n",
        "    \"ඦ\": \"ජ\",\n",
        "    \"ඤ\": \"ඥ\",\n",
        "    \"ඨ\": \"ට\",\n",
        "    \"ඪ\": \"ඩ\",\n",
        "    \"ණ\": \"න\",\n",
        "    \"ඳ\": \"ද\",\n",
        "    \"ඵ\": \"ප\",\n",
        "    \"භ\": \"බ\",\n",
        "    \"ඹ\": \"බ\",\n",
        "    \"ශ\": \"ෂ\",\n",
        "    \"ළ\": \"ල\",\n",
        "\n",
        "    # Vowels\n",
        "    \"ආ\": \"අ\",\n",
        "    \"ඈ\": \"ඇ\",\n",
        "    \"ඊ\": \"ඉ\",\n",
        "    \"ඌ\": \"උ\",\n",
        "    \"ඒ\": \"එ\",\n",
        "    \"ඕ\": \"ඔ\",\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "chcdUvABFSCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_simplified_character(character: str) -> str:\n",
        "    if len(character) != 1:\n",
        "        raise TypeError(\"character should be a string with length 1\")\n",
        "    try:\n",
        "        return simplify_characters_dict[character]\n",
        "    except KeyError:\n",
        "        return character"
      ],
      "metadata": {
        "id": "pX3yNG2CFBCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_sinhalese_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    simplify\n",
        "    :param text:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    modified_text = \"\"\n",
        "    for c in text:\n",
        "        modified_text += get_simplified_character(c)\n",
        "    return modified_text"
      ],
      "metadata": {
        "id": "7h6zI54oE1fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['filtered_sentence'] = detokenized_sentences\n",
        "df['filtered_sentence'] = df['filtered_sentence'].apply(simplify_sinhalese_text).tolist()\n",
        "#df['filtered_sentence'] = df['Text'].apply(remove_english_words).tolist()\n",
        "\n",
        "#df.to_csv('SubjectivityTagged.csv')\n",
        "df.to_excel('stem.xlsx')"
      ],
      "metadata": {
        "id": "kJp6SrbzEmXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "ygqTibDqHIH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('stem.xlsx')\n"
      ],
      "metadata": {
        "id": "ZUTdcDqaHXEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text_cleaned'] = df['Answers'].str.replace('**',' ')\n",
        "df['Text_cleaned'] = df['Text_cleaned'].str.replace('?',' ')\n",
        "\n",
        "df['Text_cleaned'][1188]"
      ],
      "metadata": {
        "id": "qOXAmr8FKYWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# english-sinhala dictionary\n",
        "dictionary = {}\n",
        "df_translate= pd.read_csv('en-sinhala dictionary.csv')\n",
        "dictionary_file = df_translate[\"En,sinhala\"]\n",
        "\n",
        "for line in dictionary_file:\n",
        "    key, value = line.strip().split(\",\")\n",
        "    dictionary[key] = value"
      ],
      "metadata": {
        "id": "lWGtQEnNOOX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to check whether the string contains English words(any)\n",
        "\n",
        "def translate_english(x):\n",
        "    for word in x.split():\n",
        "        word_cleaned = \"\".join(char for char in word if char not in string.punctuation)  # Remove punctuation\n",
        "        if re.match('[a-zA-Z]', word_cleaned):  # Check if the word contains English letters\n",
        "            word_lower = word_cleaned.lower()  # Convert to lowercase for dictionary lookup\n",
        "            translated_word = dictionary.get(word_lower, '')  # Translate using the dictionary\n",
        "            x = x.replace(word, translated_word)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Pq4TTFpNNU15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "gL9OkWtmOr6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_data(dataframe):\n",
        "    # Drop duplicate rows\n",
        "    dataframe.drop_duplicates(subset='Answers', inplace=True)\n",
        "\n",
        "    #punctuation removal\n",
        "    string_text = dataframe['Text_cleaned'].str\n",
        "    dataframe['Text_cleaned'] = string_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    print(string_text)\n",
        "\n",
        "    # coerced entire coloumn to str dtype\n",
        "    dataframe['Text_cleaned'] = dataframe['Text_cleaned'].astype(str)\n",
        "\n",
        "    #translate English to sinhala\n",
        "    df['Text_cleaned'] = df['Text_cleaned'].apply(translate_to_sinhala).tolist()\n",
        "    df['Text_cleaned'] = df['Text_cleaned'].apply(translate_english).tolist()\n",
        "\n",
        "    # simplify sinhala characters\n",
        "    df['Text_cleaned'] = df['Text_cleaned'].apply(simplify_sinhalese_text).tolist()\n",
        "\n",
        "    # pos tagging\n",
        "    #df['Text'] = df['Text'].apply(tagger.predict).tolist()\n",
        "\n",
        "    #print(\"New shape:\", dataframe.shape)\n",
        "    return dataframe.head()\n",
        "\n",
        "clean_data(df)\n",
        "df"
      ],
      "metadata": {
        "id": "gxXBGZ9MGMp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('clean.xlsx')"
      ],
      "metadata": {
        "id": "MCeHxGkTGMm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(how='all')"
      ],
      "metadata": {
        "id": "u0OAXoM6GMkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.to_excel('clean.xlsx')"
      ],
      "metadata": {
        "id": "Bdcs3H2qGMiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Labels' column to 0 and 1\n",
        "df['Label'] = df['Label'].map({'Human': 0, 'AI': 1})"
      ],
      "metadata": {
        "id": "K_CNfgn_GMfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verify the conversion\n",
        "print(df['Label'].value_counts())\n",
        "df"
      ],
      "metadata": {
        "id": "6usxEQSLGMc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download tokenizer resources\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the 'Text_cleaned' column and create a new 'Tokenized' column\n",
        "df['Tokenized'] = df['Text_cleaned'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "LSZ1n1P7GMZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['Text_cleaned', 'Tokenized']].head())"
      ],
      "metadata": {
        "id": "Pen4u26x8ILI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('tokenized_data.xlsx')"
      ],
      "metadata": {
        "id": "xIjC7sbeYLB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature **Extraction** | TF-IDF"
      ],
      "metadata": {
        "id": "PlisVEGxZ7Dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WtEY_7CU8AFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK Sinhala support (if necessary)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Tokenize Sinhala text\n",
        "df['Tokenized'] = df['Text_cleaned'].apply(word_tokenize)\n",
        "\n",
        "# Step 2: Convert tokens back to sentences for TF-IDF\n",
        "df['Text_reconstructed'] = df['Tokenized'].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "# Step 3: Initialize TF-IDF Vectorizer (customize stop words for Sinhala if needed)\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n",
        "\n",
        "# Step 4: Fit and transform the Sinhala text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Text_reconstructed'])\n",
        "\n",
        "# Step 5: Convert TF-IDF matrix to a DataFrame\n",
        "tfidf_features = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Step 6: Combine TF-IDF features with the original dataset\n",
        "df_with_tfidf = pd.concat([df, tfidf_features], axis=1)\n",
        "\n",
        "# Save the TF-IDF-enhanced dataset\n",
        "#output_file = \"/mnt/data/tfidf_features_sinhala.xlsx\"\n",
        "#df_with_tfidf.to_excel(output_file, index=False)\n",
        "\n",
        "#output_file\n"
      ],
      "metadata": {
        "id": "30mS3T4RaF7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_tfidf.head()"
      ],
      "metadata": {
        "id": "5ZljWvlGb5dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the TF-IDF-enhanced dataset\n",
        "output_file = \"tfidf_features_sinhala.xlsx\"\n",
        "df_with_tfidf.to_excel(output_file)"
      ],
      "metadata": {
        "id": "lNawsmsxb6Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = tfidf_matrix  # Features from TF-IDF\n",
        "y = df['Label']   # Labels: Human (0) or AI (1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "EuLbtUTzQK57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Ensure inputs are in array format\n",
        "X_train = np.array(X_train.toarray())  # Convert sparse matrix to dense array\n",
        "X_test = np.array(X_test.toarray())\n",
        "\n",
        "# Define the ANN model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_dim=X_train.shape[1]),  # Input layer with 128 neurons\n",
        "    Dropout(0.2),  # Dropout for regularization\n",
        "    Dense(64, activation='relu'),  # Hidden layer with 64 neurons\n",
        "    Dropout(0.2),  # Dropout for regularization\n",
        "    Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Test the model\n",
        "y_pred_prob = model.predict(X_test)  # Predicted probabilities\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGmcYfarR0sl",
        "outputId": "05d74f2f-3e2a-48b6-e7e2-08820a89a0f5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5657 - loss: 0.6864 - val_accuracy: 0.8119 - val_loss: 0.6408\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9235 - loss: 0.5174 - val_accuracy: 0.8339 - val_loss: 0.4222\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9799 - loss: 0.1241 - val_accuracy: 0.8245 - val_loss: 0.3911\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9946 - loss: 0.0303 - val_accuracy: 0.8150 - val_loss: 0.4164\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9963 - loss: 0.0156 - val_accuracy: 0.8182 - val_loss: 0.4410\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9998 - loss: 0.0056 - val_accuracy: 0.8088 - val_loss: 0.4605\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.8119 - val_loss: 0.4735\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.8056 - val_loss: 0.4914\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.8056 - val_loss: 0.4988\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9986 - loss: 0.0027 - val_accuracy: 0.8150 - val_loss: 0.5089\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "Accuracy: 0.8621553884711779\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       194\n",
            "           1       0.88      0.85      0.86       205\n",
            "\n",
            "    accuracy                           0.86       399\n",
            "   macro avg       0.86      0.86      0.86       399\n",
            "weighted avg       0.86      0.86      0.86       399\n",
            "\n"
          ]
        }
      ]
    }
  ]
}